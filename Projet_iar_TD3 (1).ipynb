{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300b203a",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code the DDPG algorithm.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The DDPG algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=0D6a0a1HTtc) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af57caf0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bbrl.agents import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f1402",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e47d6c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_shape, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_size = 256\n",
    "        \n",
    "        self.dense_1 = nn.Linear(state_shape, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_3 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = torch.tanh(self.dense_3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_shape, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_size = 256\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.dense_1 = nn.Linear(state_shape + action_dim, hidden_size)\n",
    "        self.dense_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.dense_4 = nn.Linear(state_shape + action_dim, hidden_size)\n",
    "        self.dense_5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_6 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, x_actions):\n",
    "        x = torch.cat([x, x_actions], dim=1)\n",
    "\n",
    "        q1 = F.relu(self.dense_1(x))\n",
    "        q1 = F.relu(self.dense_2(q1))\n",
    "        q1 = self.dense_3(q1)\n",
    "\n",
    "        q2 = F.relu(self.dense_4(x))\n",
    "        q2 = F.relu(self.dense_5(q2))\n",
    "        q2 = self.dense_6(q2)\n",
    "\n",
    "        return q1, q2\n",
    "    def forward_q1(self, x, x_actions):\n",
    "        x = torch.cat([x, x_actions], dim=1)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35754ad4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " ## ReplaBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9477052",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_len, state_shape, action_dim, device):\n",
    "        self.device = device\n",
    "        \n",
    "        # Maximum number of items that can be stored in buffer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Buffer\n",
    "        self.state_buffer = torch.zeros((max_len, state_shape), dtype=torch.float32).to(device)\n",
    "        self.action_buffer = torch.zeros((max_len, action_dim), dtype=torch.float32).to(device)\n",
    "        self.reward_buffer = torch.zeros(max_len, dtype=torch.float32).to(device)\n",
    "        self.next_state_buffer = torch.zeros((max_len, state_shape), dtype=torch.float32).to(device)\n",
    "        self.done_buffer = torch.zeros(max_len, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Pointer record\n",
    "        self.ptr = 0\n",
    "        \n",
    "        # Keep track of size\n",
    "        self.size = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.state_buffer[self.ptr] = torch.tensor(state).to(self.device)\n",
    "        self.action_buffer[self.ptr] = torch.tensor(action).to(self.device)\n",
    "        self.reward_buffer[self.ptr] = torch.tensor(reward).to(self.device)\n",
    "        self.next_state_buffer[self.ptr] = torch.tensor(next_state).to(self.device)\n",
    "        self.done_buffer[self.ptr] = torch.tensor(done).to(self.device)\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_len\n",
    "        self.size = min(self.size + 1, self.max_len)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, self.size, size=batch_size)\n",
    "        states = self.state_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        next_states = self.next_state_buffer[indices]\n",
    "        dones = self.done_buffer[indices]\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185116",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Gaussienne noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc23709",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class GaussianNoiseGenerator:\n",
    "    def __init__(self, sigma=0.1):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def sample(self, *args):\n",
    "        return self.sigma * np.random.randn(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb14cc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "##Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58362e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total_steps = 0\n",
    "        self.new_line_every = 25000\n",
    "        self.cumulative_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_rewards_ma = 0\n",
    "        self.episode_lengths_ma = 0\n",
    "        \n",
    "    def log(self, reward, done):\n",
    "        self.cumulative_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            self.episode_rewards.append(self.cumulative_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            self.episode_rewards_ma = np.mean(self.episode_rewards[-50:])\n",
    "            self.episode_lengths_ma = np.mean(self.episode_lengths[-50:])\n",
    "            self.cumulative_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "        self.steps += 1\n",
    "        \n",
    "    def print_logs(self):\n",
    "        end_char = \"\\n\" if self.steps % self.new_line_every == 0 else \"\\r\"\n",
    "        print(f\"Step: {self.steps}/{self.total_steps} | Avg reward per episode: {self.episode_rewards_ma:.4f} | Avg steps per episode: {self.episode_lengths_ma:.2f}\", end=end_char)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe3bcf",
   "metadata": {},
   "source": [
    "Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b18462",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate_actor': 0.001,  # Policy learning rate\n",
    "    'learning_rate_critic': 0.001, # Learning rate of value function\n",
    "    'tau': 0.005,                  # Target network update rate\n",
    "    'buffer_max_length': 500000,   # Maximum number of experiences held in replay buffer\n",
    "    'batch_size': 256,             # Number of transitions sampled per batch\n",
    "    'start_timesteps': 10000,      # Time steps initial random policy is used - 25k in original\n",
    "    'updates_per_step': 1,         # Learning updates per timestep\n",
    "    'policy_freq': 2,              # Frequency of delayed policy updates\n",
    "    'gamma': 0.99,                 # Discount factor\n",
    "    'sigma': 0.2,                  # Action noise standard deviation\n",
    "    'clip_noise': 0.5,             # Maximum noise amplitude \n",
    "    'action_low': -1,              # Lowest possible action value\n",
    "    'action_high': 1               # Highest possible action value\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ebffa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "TD3_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c63d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteActionWrapper(gym.ActionWrapper):\n",
    "    \"Bin continuous actions into discrete intervals.\"\n",
    "    def __init__(self, env, n_actions=5):\n",
    "        super().__init__(env)\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = gym.spaces.Discrete(n_actions)\n",
    "\n",
    "    def action(self, action):\n",
    "        if action == 0:\n",
    "            return np.array([-2])\n",
    "        elif action == 1:\n",
    "            return np.array([-1])\n",
    "        elif action == 2:\n",
    "            return np.array([0])\n",
    "        elif action == 3:\n",
    "            return np.array([1])\n",
    "        elif action == 4:\n",
    "            return np.array([2])\n",
    "    \n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    \"Scale the third value of the observation by 1/8.\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation[2] *= 1/8.0\n",
    "        return observation\n",
    "    \n",
    "class RewardScalingWrapper(gym.RewardWrapper):\n",
    "    \"Scale the reward by the given factor.\"\n",
    "    def __init__(self, env, scaling_factor=1/10.0):\n",
    "        super().__init__(env)\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * self.scaling_factor\n",
    "    \n",
    "env = gym.make('LunarLander-v2')\n",
    "env = RewardScalingWrapper(DiscreteActionWrapper(ObservationWrapper(env)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e1548-51d7-48cd-8822-a943e3f666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CleanRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4ba0a6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, env, Actor, Critic, config):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            env (gym.Env): A gym environment\n",
    "            Actor (torch.nn.Module): Policy neural network class\n",
    "            Critic (torch.nn.Module): Q-function neural network class\n",
    "            config (dict): A dictionary containing the TD3 configuration settings\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_shape = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.actor = Actor(self.state_shape, self.action_dim).to(self.device)\n",
    "        self.critic = Critic(self.state_shape, self.action_dim).to(self.device)\n",
    "\n",
    "        self.actor_target = Actor(self.state_shape, self.action_dim).to(self.device)\n",
    "        self.critic_target = Critic(self.state_shape, self.action_dim).to(self.device)\n",
    "        \n",
    "        # Configuration\n",
    "        self.learning_rate_actor = config['learning_rate_actor']\n",
    "        self.learning_rate_critic = config['learning_rate_critic']\n",
    "        self.tau = config['tau']\n",
    "        self.buffer_max_length = config['buffer_max_length']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.start_timesteps = config['start_timesteps']\n",
    "        self.updates_per_step = config['updates_per_step']\n",
    "        self.policy_freq = config['policy_freq']\n",
    "        self.gamma = config['gamma']\n",
    "        self.sigma = config['sigma']\n",
    "        self.clip_noise = config['clip_noise']\n",
    "        self.action_low = config['action_low']\n",
    "        self.action_high = config['action_high']\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.learning_rate_actor)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.learning_rate_critic)\n",
    "        \n",
    "        # Initialize replay buffer\n",
    "        self.buffer = ReplayBuffer(self.buffer_max_length, self.state_shape, self.action_dim, self.device)\n",
    "\n",
    "        # Gaussian noise\n",
    "        self.gaussian_noise = GaussianNoiseGenerator(self.sigma)\n",
    "        \n",
    "        # Set target network parameters\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Logging\n",
    "        self.logger = Logger()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Predict a single action from a state without gradient computation.\"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.actor(state_tensor).detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "    def train_agent(self,seed,  max_timesteps=250000):\n",
    "        \n",
    "        # Reset environment\n",
    "    \n",
    "        state, info = env.reset(seed=seed)\n",
    "        \n",
    "        # Store \n",
    "        self.logger.total_steps = max_timesteps\n",
    "        \n",
    "        for current_timestep in range(max_timesteps):\n",
    "            \n",
    "            # Exploring start\n",
    "            if len(self.buffer) < self.start_timesteps:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.act(state)\n",
    "                noise = self.gaussian_noise.sample(self.action_dim)\n",
    "                action = np.clip(action + noise, a_min=self.action_low, a_max=self.action_high)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            self.buffer.append(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Log reward and done, then print progress\n",
    "            self.logger.log(reward, done)\n",
    "            self.logger.print_logs()\n",
    "            \n",
    "            # Reset environment if done, else prepare state for next iteration\n",
    "            if done:\n",
    "                state, info = self.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            if current_timestep < self.start_timesteps:\n",
    "                continue\n",
    "                \n",
    "            # Sample from replay buffer\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "            # Compute target for critic loss calculation\n",
    "            with torch.no_grad():\n",
    "                noise = torch.tensor(self.gaussian_noise.sample(self.batch_size, self.action_dim), dtype=torch.float, device=self.device).clip(-self.clip_noise, self.clip_noise)\n",
    "                \n",
    "                # Clipped actions + noise\n",
    "                next_actions = torch.clip(self.actor_target(next_states) + noise, min=self.action_low, max=self.action_high)\n",
    "\n",
    "                targets_q1, targets_q2 = self.critic(next_states, next_actions)\n",
    "                targets_q = torch.min(targets_q1, targets_q2).squeeze(-1)\n",
    "                targets_q = rewards + self.gamma * (1 - dones) * targets_q\n",
    "\n",
    "            # Current Q predictions\n",
    "            pred_q1, pred_q2 = self.critic(states, actions)\n",
    "\n",
    "            # Critic loss\n",
    "            loss_critic = F.mse_loss(pred_q1.squeeze(-1), targets_q) + F.mse_loss(pred_q2.squeeze(-1), targets_q)\n",
    "\n",
    "            # Critic backward pass\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "            # Delayed policy learning updates\n",
    "            if current_timestep % self.policy_freq == 0:\n",
    "                \n",
    "                # Actor loss\n",
    "                loss_actor = -self.critic.forward_q1(states, self.actor(states)).mean()\n",
    "                \n",
    "                # Actor backward pass\n",
    "                self.optimizer_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.optimizer_actor.step()\n",
    "                \n",
    "                # Update target network parameters\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae0995",
   "metadata": {},
   "source": [
    "### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb6e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu9/3603679/.local/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 25000/250000 | Avg reward per episode: -10.6694 | Avg steps per episode: 312.88\n",
      "Step: 47709/250000 | Avg reward per episode: -2.2112 | Avg steps per episode: 502.768\r"
     ]
    }
   ],
   "source": [
    "# Crée l'environnement\n",
    "env = gym.make('LunarLanderContinuous-v2')  # Utilise l'environnement continu\n",
    "env = RewardScalingWrapper(ObservationWrapper(env))\n",
    "\n",
    "# Crée l'agent TD3\n",
    "agent_lunar = TD3Agent(env, Actor, Critic, params)\n",
    "\n",
    "# Entraîne l'agent\n",
    "seed = 42  # Utilise un seed pour la reproductibilité, par exemple\n",
    "agent_lunar.train_agent(seed, 250000)\n",
    "def plot_results(steps, avg_rewards, avg_steps):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Courbe des récompenses moyennes\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, avg_rewards, marker='o', color='b')\n",
    "    plt.title('Récompense Moyenne par Épisode')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Récompense Moyenne')\n",
    "    plt.grid()\n",
    "\n",
    "    # Courbe des étapes moyennes\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, avg_steps, marker='o', color='r')\n",
    "    plt.title('Étapes Moyennes par Épisode')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Étapes Moyennes')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tracer les résultats\n",
    "plot_results(agent_lunar.steps, agent_lunar.avg_rewards, agent_lunar.avg_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066fb6f8-e404-42c1-92b5-6648372e2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, env, Actor, Critic, config):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            env (gym.Env): A gym environment\n",
    "            Actor (torch.nn.Module): Policy neural network class\n",
    "            Critic (torch.nn.Module): Q-function neural network class\n",
    "            config (dict): A dictionary containing the TD3 configuration settings\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_shape = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.actor = Actor(self.state_shape, self.action_dim).to(self.device)\n",
    "        self.critic = Critic(self.state_shape, self.action_dim).to(self.device)\n",
    "\n",
    "        self.actor_target = Actor(self.state_shape, self.action_dim).to(self.device)\n",
    "        self.critic_target = Critic(self.state_shape, self.action_dim).to(self.device)\n",
    "        \n",
    "        # Configuration\n",
    "        self.learning_rate_actor = config['learning_rate_actor']\n",
    "        self.learning_rate_critic = config['learning_rate_critic']\n",
    "        self.tau = config['tau']\n",
    "        self.buffer_max_length = config['buffer_max_length']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.start_timesteps = config['start_timesteps']\n",
    "        self.updates_per_step = config['updates_per_step']\n",
    "        self.policy_freq = config['policy_freq']\n",
    "        self.gamma = config['gamma']\n",
    "        self.sigma = config['sigma']\n",
    "        self.clip_noise = config['clip_noise']\n",
    "        self.action_low = config['action_low']\n",
    "        self.action_high = config['action_high']\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.learning_rate_actor)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.learning_rate_critic)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.buffer = ReplayBuffer(self.buffer_max_length, self.state_shape, self.action_dim, self.device)\n",
    "\n",
    "        # Gaussian noise\n",
    "        self.gaussian_noise = GaussianNoiseGenerator(self.sigma)\n",
    "        \n",
    "        # Set target network parameters\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Logging\n",
    "        self.logger = Logger()\n",
    "        \n",
    "    def train(self, max_timesteps):\n",
    "        \"\"\"Trains the agent for a maximum number of timesteps.\"\"\"\n",
    "        # Reset environment\n",
    "        state, info = self.env.reset()\n",
    "        \n",
    "        # Store \n",
    "        self.logger.total_steps = max_timesteps\n",
    "        \n",
    "        for current_timestep in range(max_timesteps):\n",
    "            # Exploring start\n",
    "            if len(self.buffer) < self.start_timesteps:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.act(state)\n",
    "                noise = self.gaussian_noise.sample(self.action_dim)\n",
    "                action = np.clip(action + noise, a_min=self.action_low, a_max=self.action_high)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            self.buffer.append(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Log reward and done, then print progress\n",
    "            self.logger.log(reward, done)\n",
    "            self.logger.print_logs()\n",
    "\n",
    "            # Reset environment if done, else prepare state for next iteration\n",
    "            if done:\n",
    "                state, info = self.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            if current_timestep < self.start_timesteps:\n",
    "                continue\n",
    "            \n",
    "            # Sample from replay buffer\n",
    "            states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "            # Compute target for critic loss calculation\n",
    "            with torch.no_grad():\n",
    "                # Clipped noise\n",
    "                noise = torch.tensor(self.gaussian_noise.sample(self.batch_size, self.action_dim), \n",
    "                                     dtype=torch.float, device=self.device).clip(-self.clip_noise, self.clip_noise)\n",
    "                \n",
    "                # Clipped actions + noise\n",
    "                next_actions = torch.clip(self.actor_target(next_states) + noise, min=self.action_low, max=self.action_high)\n",
    "\n",
    "                targets_q1, targets_q2 = self.critic(next_states, next_actions)\n",
    "                targets_q = torch.min(targets_q1, targets_q2).squeeze(-1)\n",
    "                targets_q = rewards + self.gamma * (1 - dones) * targets_q\n",
    "\n",
    "            # Current Q predictions\n",
    "            pred_q1, pred_q2 = self.critic(states, actions)\n",
    "\n",
    "            # Critic loss\n",
    "            loss_critic = F.mse_loss(pred_q1.squeeze(-1), targets_q) + F.mse_loss(pred_q2.squeeze(-1), targets_q)\n",
    "\n",
    "            # Critic backward pass\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "            # Delayed policy learning updates\n",
    "            if current_timestep % self.policy_freq == 0:\n",
    "                # Actor loss\n",
    "                loss_actor = -self.critic.forward_q1(states, self.actor(states)).mean()\n",
    "                \n",
    "                # Actor backward pass\n",
    "                self.optimizer_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.optimizer_actor.step()\n",
    "                \n",
    "                # Update target network parameters\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Predict a single action from a state without gradient computation.\"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.actor(state_tensor).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410c9368-52ca-45f3-8d77-cd1cc8a88f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu9/3603679/.local/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 25000/250000 | Avg reward per episode: -2.2762 | Avg steps per episode: 312.406\n",
      "Step: 50000/250000 | Avg reward per episode: 24.5511 | Avg steps per episode: 376.00\n",
      "Step: 75000/250000 | Avg reward per episode: 23.3703 | Avg steps per episode: 291.06\n",
      "Step: 100000/250000 | Avg reward per episode: 23.8254 | Avg steps per episode: 285.24\n",
      "Step: 125000/250000 | Avg reward per episode: 27.1390 | Avg steps per episode: 256.64\n",
      "Step: 150000/250000 | Avg reward per episode: 27.1703 | Avg steps per episode: 239.98\n",
      "Step: 175000/250000 | Avg reward per episode: 25.8402 | Avg steps per episode: 266.32\n",
      "Step: 200000/250000 | Avg reward per episode: 25.2306 | Avg steps per episode: 263.06\n",
      "Step: 225000/250000 | Avg reward per episode: 25.7221 | Avg steps per episode: 254.24\n",
      "Step: 250000/250000 | Avg reward per episode: 27.6247 | Avg steps per episode: 242.64\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=True)\n",
    "env = RewardScalingWrapper(ObservationWrapper(env))\n",
    "\n",
    "agent_lunar = TD3Agent(env, Actor, Critic, params)\n",
    "agent_lunar.train(250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12d4ec-13c6-4706-9acf-d991851d6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### stable_baselines3: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511c76fa-a782-4e94-a067-ed02e5429895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Type, Union\n",
    "\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from torch import nn\n",
    "import gym\n",
    "\n",
    "from stable_baselines3.common.policies import BasePolicy, ContinuousCritic\n",
    "from stable_baselines3.common.preprocessing import get_action_dim\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    NatureCNN,\n",
    "    create_mlp,\n",
    "    get_actor_critic_arch,\n",
    ")\n",
    "from stable_baselines3.common.type_aliases import PyTorchObs, Schedule\n",
    "\n",
    "\n",
    "\n",
    "class Actor(BasePolicy):\n",
    "    \"\"\"\n",
    "    Actor network (policy) for TD3.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param net_arch: Network architecture\n",
    "    :param features_extractor: Network to extract features\n",
    "        (a CNN when using images, a nn.Flatten() layer otherwise)\n",
    "    :param features_dim: Number of features\n",
    "    :param activation_fn: Activation function\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Box,\n",
    "        net_arch: List[int],\n",
    "        features_extractor: nn.Module,\n",
    "        features_dim: int,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        normalize_images: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_extractor=features_extractor,\n",
    "            normalize_images=normalize_images,\n",
    "            squash_output=True,\n",
    "        )\n",
    "\n",
    "        self.net_arch = net_arch\n",
    "        self.features_dim = features_dim\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        action_dim = get_action_dim(self.action_space)\n",
    "        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=True)\n",
    "        # Deterministic action\n",
    "        self.mu = nn.Sequential(*actor_net)\n",
    "\n",
    "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                net_arch=self.net_arch,\n",
    "                features_dim=self.features_dim,\n",
    "                activation_fn=self.activation_fn,\n",
    "                features_extractor=self.features_extractor,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def forward(self, obs: th.Tensor) -> th.Tensor:\n",
    "        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n",
    "        features = self.extract_features(obs, self.features_extractor)\n",
    "        return self.mu(features)\n",
    "\n",
    "    def _predict(self, observation: PyTorchObs, deterministic: bool = False) -> th.Tensor:\n",
    "        # Note: the deterministic deterministic parameter is ignored in the case of TD3.\n",
    "        #   Predictions are always deterministic.\n",
    "        return self(observation)\n",
    "\n",
    "\n",
    "class TD3Policy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Policy class (with both actor and critic) for TD3.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    :param n_critics: Number of critic networks to create.\n",
    "    :param share_features_extractor: Whether to share or not the features extractor\n",
    "        between the actor and the critic (this saves computation time)\n",
    "    \"\"\"\n",
    "\n",
    "    actor: Actor\n",
    "    actor_target: Actor\n",
    "    critic: ContinuousCritic\n",
    "    critic_target: ContinuousCritic\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Box,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        n_critics: int = 2,\n",
    "        share_features_extractor: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            optimizer_class=optimizer_class,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            squash_output=True,\n",
    "            normalize_images=normalize_images,\n",
    "        )\n",
    "\n",
    "        # Default network architecture, from the original paper\n",
    "        if net_arch is None:\n",
    "            if features_extractor_class == NatureCNN:\n",
    "                net_arch = [256, 256]\n",
    "            else:\n",
    "                net_arch = [400, 300]\n",
    "\n",
    "        actor_arch, critic_arch = get_actor_critic_arch(net_arch)\n",
    "\n",
    "        self.net_arch = net_arch\n",
    "        self.activation_fn = activation_fn\n",
    "        self.net_args = {\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"net_arch\": actor_arch,\n",
    "            \"activation_fn\": self.activation_fn,\n",
    "            \"normalize_images\": normalize_images,\n",
    "        }\n",
    "        self.actor_kwargs = self.net_args.copy()\n",
    "        self.critic_kwargs = self.net_args.copy()\n",
    "        self.critic_kwargs.update(\n",
    "            {\n",
    "                \"n_critics\": n_critics,\n",
    "                \"net_arch\": critic_arch,\n",
    "                \"share_features_extractor\": share_features_extractor,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.share_features_extractor = share_features_extractor\n",
    "\n",
    "        self._build(lr_schedule)\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        # Create actor and target\n",
    "        # the features extractor should not be shared\n",
    "        self.actor = self.make_actor(features_extractor=None)\n",
    "        self.actor_target = self.make_actor(features_extractor=None)\n",
    "        # Initialize the target to have the same weights as the actor\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.actor.optimizer = self.optimizer_class(\n",
    "            self.actor.parameters(),\n",
    "            lr=lr_schedule(1),  # type: ignore[call-arg]\n",
    "            **self.optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "        if self.share_features_extractor:\n",
    "            self.critic = self.make_critic(features_extractor=self.actor.features_extractor)\n",
    "            # Critic target should not share the features extractor with critic\n",
    "            # but it can share it with the actor target as actor and critic are sharing\n",
    "            # the same features_extractor too\n",
    "            # NOTE: as a result the effective poliak (soft-copy) coefficient for the features extractor\n",
    "            # will be 2 * tau instead of tau (updated one time with the actor, a second time with the critic)\n",
    "            self.critic_target = self.make_critic(features_extractor=self.actor_target.features_extractor)\n",
    "        else:\n",
    "            # Create new features extractor for each network\n",
    "            self.critic = self.make_critic(features_extractor=None)\n",
    "            self.critic_target = self.make_critic(features_extractor=None)\n",
    "\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic.optimizer = self.optimizer_class(\n",
    "            self.critic.parameters(),\n",
    "            lr=lr_schedule(1),  # type: ignore[call-arg]\n",
    "            **self.optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "        # Target networks should always be in eval mode\n",
    "        self.actor_target.set_training_mode(False)\n",
    "        self.critic_target.set_training_mode(False)\n",
    "\n",
    "    def _get_constructor_parameters(self) -> Dict[str, Any]:\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                net_arch=self.net_arch,\n",
    "                activation_fn=self.net_args[\"activation_fn\"],\n",
    "                n_critics=self.critic_kwargs[\"n_critics\"],\n",
    "                lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n",
    "                optimizer_class=self.optimizer_class,\n",
    "                optimizer_kwargs=self.optimizer_kwargs,\n",
    "                features_extractor_class=self.features_extractor_class,\n",
    "                features_extractor_kwargs=self.features_extractor_kwargs,\n",
    "                share_features_extractor=self.share_features_extractor,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None) -> Actor:\n",
    "        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n",
    "        return Actor(**actor_kwargs).to(self.device)\n",
    "\n",
    "    def make_critic(self, features_extractor: Optional[BaseFeaturesExtractor] = None) -> ContinuousCritic:\n",
    "        critic_kwargs = self._update_features_extractor(self.critic_kwargs, features_extractor)\n",
    "        return ContinuousCritic(**critic_kwargs).to(self.device)\n",
    "\n",
    "    def forward(self, observation: PyTorchObs, deterministic: bool = False) -> th.Tensor:\n",
    "        return self._predict(observation, deterministic=deterministic)\n",
    "\n",
    "    def _predict(self, observation: PyTorchObs, deterministic: bool = False) -> th.Tensor:\n",
    "        # Note: the deterministic deterministic parameter is ignored in the case of TD3.\n",
    "        #   Predictions are always deterministic.\n",
    "        return self.actor(observation)\n",
    "\n",
    "    def set_training_mode(self, mode: bool) -> None:\n",
    "        \"\"\"\n",
    "        Put the policy in either training or evaluation mode.\n",
    "\n",
    "        This affects certain modules, such as batch normalisation and dropout.\n",
    "\n",
    "        :param mode: if true, set to training mode, else set to evaluation mode\n",
    "        \"\"\"\n",
    "        self.actor.set_training_mode(mode)\n",
    "        self.critic.set_training_mode(mode)\n",
    "        self.training = mode\n",
    "\n",
    "\n",
    "MlpPolicy = TD3Policy\n",
    "\n",
    "\n",
    "class CnnPolicy(TD3Policy):\n",
    "    \"\"\"\n",
    "    Policy class (with both actor and critic) for TD3.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    :param n_critics: Number of critic networks to create.\n",
    "    :param share_features_extractor: Whether to share or not the features extractor\n",
    "        between the actor and the critic (this saves computation time)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Box,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        n_critics: int = 2,\n",
    "        share_features_extractor: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs,\n",
    "            n_critics,\n",
    "            share_features_extractor,\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiInputPolicy(TD3Policy):\n",
    "    \"\"\"\n",
    "    Policy class (with both actor and critic) for TD3 to be used with Dict observation spaces.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    :param n_critics: Number of critic networks to create.\n",
    "    :param share_features_extractor: Whether to share or not the features extractor\n",
    "        between the actor and the critic (this saves computation time)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Dict,\n",
    "        action_space: spaces.Box,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = CombinedExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        n_critics: int = 2,\n",
    "        share_features_extractor: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs,\n",
    "            n_critics,\n",
    "            share_features_extractor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a3478-f6d1-442c-b747-ddb34bbc7bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu9/3603679/.local/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 432      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.8      |\n",
      "|    critic_loss     | 92.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 331      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 746      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.6     |\n",
      "|    critic_loss     | 47.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 645      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1069     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 74.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 968      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1581     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.6     |\n",
      "|    critic_loss     | 51.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2072     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.8     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1971     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2603     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.4     |\n",
      "|    critic_loss     | 44.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2502     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.1     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2959     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 3572     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 9.48     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3471     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 4087     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24       |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3986     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 4782     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4681     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5201     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 10       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5100     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 5655     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.7     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5554     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 6276     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.3     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6175     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 7020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6919     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 7650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 17.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7549     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 8072     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 69.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7971     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 8590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8489     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 9155     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 6.02     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9054     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 9799     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9698     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 10522    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10421    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 11524    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.8     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11423    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 12774    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.7     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12673    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 13911    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 14755    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 9.14     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14654    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 15589    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 87.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15488    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 16600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.3     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 17276    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.6     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17175    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 18283    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.4     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18182    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 18902    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18801    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 19729    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.7     |\n",
      "|    critic_loss     | 26       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19628    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 20692    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.1     |\n",
      "|    critic_loss     | 8.66     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20591    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 21725    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.9     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21624    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 22879    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.6     |\n",
      "|    critic_loss     | 12       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22778    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 23672    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.4     |\n",
      "|    critic_loss     | 120      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23571    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 24546    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.5     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24445    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 26098    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.8     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25997    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 28494    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.9     |\n",
      "|    critic_loss     | 7.91     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28393    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 30295    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.3     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30194    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 31722    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.1     |\n",
      "|    critic_loss     | 8.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31621    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 33066    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.3     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 32965    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 34256    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.8     |\n",
      "|    critic_loss     | 9.41     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34155    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 36796    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29       |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36695    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 38768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.6     |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38667    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 41821    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.8     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 41720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 45821    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31       |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 49821    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.6     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 53541    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.4     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 53440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 399      |\n",
      "|    total_timesteps | 57370    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 30.7     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 57269    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total_timesteps | 61370    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29       |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61269    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 63079    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 31.3     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 62978    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch as th\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv  # Corrigé ici\n",
    "\n",
    "# Créez un environnement\n",
    "env_id = 'LunarLanderContinuous-v2'\n",
    "env = gym.make(env_id)\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment\n",
    "\n",
    "# Initialisez le modèle TD3\n",
    "model = TD3('MlpPolicy', env, verbose=1,)\n",
    "\n",
    "# Entraînez le modèle\n",
    "model.learn(total_timesteps=250000)\n",
    "\n",
    "# Sauvegardez le modèle\n",
    "model.save(\"td3_lunarlander\")\n",
    "\n",
    "# Évaluer le modèle\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9462b24-1f13-4547-9135-0ce40d47124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### stable_baselines3: TD3-version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eac168-71b3-4f46-8c63-505ec073bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début des tests d'hyperparamètres...\n",
      "\n",
      "Test 1/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 64, 'policy_delay': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu9/3603679/.local/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récompense moyenne: -49.84 +/- 34.61\n",
      "\n",
      "Test 2/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 64, 'policy_delay': 2}\n",
      "Récompense moyenne: -44.38 +/- 24.61\n",
      "\n",
      "Test 3/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 64, 'policy_delay': 4}\n",
      "Récompense moyenne: -29.36 +/- 115.69\n",
      "\n",
      "Test 4/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 128, 'policy_delay': 1}\n",
      "Récompense moyenne: 23.24 +/- 109.18\n",
      "\n",
      "Test 5/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 128, 'policy_delay': 2}\n",
      "Récompense moyenne: -107.40 +/- 22.91\n",
      "\n",
      "Test 6/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 128, 'policy_delay': 4}\n",
      "Récompense moyenne: -83.05 +/- 32.03\n",
      "\n",
      "Test 7/81\n",
      "Paramètres: {'learning_rate': 0.0001, 'tau': 0.001, 'batch_size': 256, 'policy_delay': 1}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def test_hyperparameters():\n",
    "    # Définition des hyperparamètres à tester\n",
    "    hyperparameters = {\n",
    "        'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "        'tau': [0.001, 0.005, 0.01],\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'policy_delay': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Création de l'environnement\n",
    "    env = DummyVecEnv([lambda: gym.make('LunarLanderContinuous-v2')])\n",
    "    \n",
    "    # Création du bruit d'action\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "    # Génération de toutes les combinaisons d'hyperparamètres\n",
    "    keys = hyperparameters.keys()\n",
    "    values = hyperparameters.values()\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    results = []\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"\\nTest {i + 1}/{len(combinations)}\")\n",
    "        print(f\"Paramètres: {params}\")\n",
    "        \n",
    "        # Création et entraînement du modèle\n",
    "        model = TD3('MlpPolicy', env, action_noise=action_noise, verbose=0, **params)\n",
    "        try:\n",
    "            model.learn(total_timesteps=100000)  # Réduit pour le test\n",
    "            \n",
    "            # Évaluation\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "            \n",
    "            results.append({\n",
    "                **params,\n",
    "                'mean_reward': mean_reward,\n",
    "                'std_reward': std_reward\n",
    "            })\n",
    "            print(f\"Récompense moyenne: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec les paramètres {params}: {e}\")\n",
    "            continue  # Continue to the next set of hyperparameters\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    print(\"Début des tests d'hyperparamètres...\")\n",
    "    results_df = test_hyperparameters()\n",
    "    \n",
    "    # Trier et afficher les résultats\n",
    "    results_df = results_df.sort_values('mean_reward', ascending=False)\n",
    "    print(\"\\nRésultats triés par récompense moyenne:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    results_df.to_csv('td3_hyperparameter_results.csv', index=False)\n",
    "    print(\"\\nRésultats sauvegardés dans 'td3_hyperparameter_results.csv'\")\n",
    "    \n",
    "    # Afficher les meilleurs paramètres\n",
    "    if not results_df.empty:\n",
    "        best_params = results_df.iloc[0].to_dict()\n",
    "        print(\"\\nMeilleurs paramètres trouvés:\")\n",
    "        for key, value in best_params.items():\n",
    "            if key not in ['mean_reward', 'std_reward']:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(f\"Récompense moyenne: {best_params['mean_reward']:.2f} +/- {best_params['std_reward']:.2f}\")\n",
    "    else:\n",
    "        print(\"Aucun résultat valide trouvé.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36e9d4-dce6-4800-81a3-6cd4210267fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logs d'entraînement\n",
    "log_data = \"\"\"\n",
    "... (insérer ici les logs d'entraînement) ...\n",
    "\"\"\"\n",
    "\n",
    "# Initialiser les listes pour les pertes\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "n_updates = []\n",
    "\n",
    "# Expression régulière pour extraire les pertes\n",
    "for line in log_data.splitlines():\n",
    "    if 'actor_loss' in line:\n",
    "        actor_loss = float(re.search(r'actor_loss\\s+([\\d.-]+)', line).group(1))\n",
    "        critic_loss = float(re.search(r'critic_loss\\s+([\\d.-]+)', line).group(1))\n",
    "        n_update = int(re.search(r'n_updates\\s+(\\d+)', line).group(1))\n",
    "        \n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "        n_updates.append(n_update)\n",
    "\n",
    "# Afficher les données pour vérifier\n",
    "print(\"Actor losses:\", actor_losses)\n",
    "print(\"Critic losses:\", critic_losses)\n",
    "print(\"Number of updates:\", n_updates)\n",
    "# Tracer les pertes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_updates, actor_losses, label='Actor Loss', color='blue', marker='o')\n",
    "plt.plot(n_updates, critic_losses, label='Critic Loss', color='red', marker='x')\n",
    "\n",
    "plt.title('Actor and Critic Losses Over Time')\n",
    "plt.xlabel('Number of Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d27986-1214-4df3-bbdc-780ed6780c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### stable_baselines3: TD3-version2-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da1cff-f293-4b9e-9dd5-be8998e89d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def test_hyperparameters():\n",
    "    # Définition des hyperparamètres à tester\n",
    "    hyperparameters = {\n",
    "        'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "        'tau': [0.001, 0.005, 0.01],\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'policy_delay': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Création de l'environnement\n",
    "    env = DummyVecEnv([lambda: gym.make('LunarLanderContinuous-v2')])\n",
    "    \n",
    "    # Création du bruit d'action\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "    # Génération de toutes les combinaisons d'hyperparamètres\n",
    "    keys = hyperparameters.keys()\n",
    "    values = hyperparameters.values()\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    results = []\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"\\nTest {i + 1}/{len(combinations)}\")\n",
    "        print(f\"Paramètres: {params}\")\n",
    "        \n",
    "        # Création et entraînement du modèle\n",
    "        model = TD3('MlpPolicy', env, action_noise=action_noise, verbose=0, **params)\n",
    "        try:\n",
    "            model.learn(total_timesteps=100000)  # Réduit pour le test\n",
    "            \n",
    "            # Évaluation\n",
    "            mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "            \n",
    "            results.append({\n",
    "                **params,\n",
    "                'mean_reward': mean_reward,\n",
    "                'std_reward': std_reward\n",
    "            })\n",
    "            print(f\"Récompense moyenne: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec les paramètres {params}: {e}\")\n",
    "            continue  # Continue to the next set of hyperparameters\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    print(\"Début des tests d'hyperparamètres...\")\n",
    "    results_df = test_hyperparameters()\n",
    "    \n",
    "    # Trier et afficher les résultats\n",
    "    results_df = results_df.sort_values('mean_reward', ascending=False)\n",
    "    print(\"\\nRésultats triés par récompense moyenne:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    results_df.to_csv('td3_hyperparameter_results.csv', index=False)\n",
    "    print(\"\\nRésultats sauvegardés dans 'td3_hyperparameter_results.csv'\")\n",
    "    \n",
    "    # Afficher les meilleurs paramètres\n",
    "    if not results_df.empty:\n",
    "        best_params = results_df.iloc[0].to_dict()\n",
    "        print(\"\\nMeilleurs paramètres trouvés:\")\n",
    "        for key, value in best_params.items():\n",
    "            if key not in ['mean_reward', 'std_reward']:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(f\"Récompense moyenne: {best_params['mean_reward']:.2f} +/- {best_params['std_reward']:.2f}\")\n",
    "    else:\n",
    "        print(\"Aucun résultat valide trouvé.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8de03b-1835-4332-93fd-8de877ead5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Taishu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "250f80a4-d096-4cc9-a36e-2d99602cb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from tianshou.policy import DDPGPolicy\n",
    "\n",
    "class TD3Policy(DDPGPolicy):\n",
    "    \n",
    "    def __init__(self, actor, actor_optim, critic1, critic1_optim,\n",
    "                 critic2, critic2_optim, tau=0.005, gamma=0.99,\n",
    "                 exploration_noise=0.1, policy_noise=0.2, update_actor_freq=2,\n",
    "                 noise_clip=0.5, action_range=None,\n",
    "                 reward_normalization=False, ignore_done=False, **kwargs):\n",
    "        super().__init__(actor, actor_optim, None, None, tau, gamma,\n",
    "                         exploration_noise, action_range, reward_normalization,\n",
    "                         ignore_done)\n",
    "        self.critic1, self.critic1_old = critic1, deepcopy(critic1)\n",
    "        self.critic1_old.eval()\n",
    "        self.critic1_optim = critic1_optim\n",
    "        self.critic2, self.critic2_old = critic2, deepcopy(critic2)\n",
    "        self.critic2_old.eval()\n",
    "        self.critic2_optim = critic2_optim\n",
    "        self._policy_noise = policy_noise\n",
    "        self._freq = update_actor_freq\n",
    "        self._noise_clip = noise_clip\n",
    "        self._cnt = 0\n",
    "        self._last = 0\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        self.actor.train()\n",
    "        self.critic1.train()\n",
    "        self.critic2.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        self.actor.eval()\n",
    "        self.critic1.eval()\n",
    "        self.critic2.eval()\n",
    "\n",
    "    def sync_weight(self):\n",
    "        for o, n in zip(self.actor_old.parameters(), self.actor.parameters()):\n",
    "            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n",
    "        for o, n in zip(self.critic1_old.parameters(), self.critic1.parameters()):\n",
    "            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n",
    "        for o, n in zip(self.critic2_old.parameters(), self.critic2.parameters()):\n",
    "            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            a_ = self(batch, model='actor_old', input='obs_next').act\n",
    "            dev = a_.device\n",
    "            noise = torch.randn(size=a_.shape, device=dev) * self._policy_noise\n",
    "            if self._noise_clip >= 0:\n",
    "                noise = noise.clamp(-self._noise_clip, self._noise_clip)\n",
    "            a_ += noise\n",
    "            a_ = a_.clamp(self._range[0], self._range[1])\n",
    "            target_q = torch.min(\n",
    "                self.critic1_old(batch.obs_next, a_),\n",
    "                self.critic2_old(batch.obs_next, a_)\n",
    "            )\n",
    "            rew = torch.tensor(batch.rew, dtype=torch.float, device=dev)[:, None]\n",
    "            done = torch.tensor(batch.done, dtype=torch.float, device=dev)[:, None]\n",
    "            target_q = (rew + (1. - done) * self._gamma * target_q)\n",
    "\n",
    "        # Critic 1\n",
    "        current_q1 = self.critic1(batch.obs, batch.act)\n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        self.critic1_optim.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optim.step()\n",
    "\n",
    "        # Critic 2\n",
    "        current_q2 = self.critic2(batch.obs, batch.act)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "        self.critic2_optim.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optim.step()\n",
    "\n",
    "        if self._cnt % self._freq == 0:\n",
    "            actor_loss = -self.critic1(batch.obs, self(batch, eps=0).act).mean()\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self._last = actor_loss.item()\n",
    "            self.actor_optim.step()\n",
    "            self.sync_weight()\n",
    "\n",
    "        self._cnt += 1\n",
    "        return {\n",
    "            'loss/actor': self._last,\n",
    "            'loss/critic1': critic1_loss.item(),\n",
    "            'loss/critic2': critic2_loss.item(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5db0fcf-b9f3-49bc-bcf3-c7aba46e4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tianshou.policy import DDPGPolicy\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Supposons que TD3Policy est définie ici\n",
    "class TD3Policy(DDPGPolicy):\n",
    "    # Votre code TD3Policy ici...\n",
    "    def __init__(self, actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim, **kwargs):\n",
    "        super().__init__(actor, actor_optim, None, None, **kwargs)\n",
    "        self.critic1 = critic1\n",
    "        self.critic1_optim = critic1_optim\n",
    "        self.critic2 = critic2\n",
    "        self.critic2_optim = critic2_optim\n",
    "        # Initialiser d'autres attributs nécessaires\n",
    "\n",
    "    # Ajoutez les méthodes nécessaires comme `learn`, `sync_weight`, etc.\n",
    "\n",
    "def test_hyperparameters():\n",
    "    # Définition des hyperparamètres à tester\n",
    "    hyperparameters = {\n",
    "        'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "        'tau': [0.001, 0.005, 0.01],\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'policy_delay': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Création de l'environnement\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    \n",
    "    # Création des modèles d'acteurs et de critiques\n",
    "    actor = nn.Sequential(nn.Linear(8, 256), nn.ReLU(), nn.Linear(256, 2))\n",
    "    critic1 = nn.Sequential(nn.Linear(8 + 2, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "    critic2 = nn.Sequential(nn.Linear(8 + 2, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "    \n",
    "    results = []\n",
    "    param_combinations = list(itertools.product(*hyperparameters.values()))\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        param_dict = dict(zip(hyperparameters.keys(), params))\n",
    "        print(f\"\\nTest {i + 1}/{len(param_combinations)}\")\n",
    "        print(f\"Paramètres: {param_dict}\")\n",
    "\n",
    "        # Création et entraînement du modèle\n",
    "        actor_optim = Adam(actor.parameters(), lr=param_dict['learning_rate'])\n",
    "        critic1_optim = Adam(critic1.parameters(), lr=param_dict['learning_rate'])\n",
    "        critic2_optim = Adam(critic2.parameters(), lr=param_dict['learning_rate'])\n",
    "        \n",
    "        model = TD3Policy(actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim, **param_dict)\n",
    "        \n",
    "        try:\n",
    "            for _ in range(1000):  # Boucle d'entraînement simplifiée\n",
    "                # Simulez un batch d'entraînement ici\n",
    "                obs = np.random.rand(32, 8)  # Exemples d'observations\n",
    "                actions = np.random.rand(32, 2)  # Actions aléatoires\n",
    "                rewards = np.random.rand(32)  # Récompenses aléatoires\n",
    "                dones = np.random.randint(0, 2, size=(32,))  # Terminaison aléatoire\n",
    "\n",
    "                batch = {\n",
    "                    'obs': torch.tensor(obs, dtype=torch.float32),\n",
    "                    'act': torch.tensor(actions, dtype=torch.float32),\n",
    "                    'rew': torch.tensor(rewards, dtype=torch.float32),\n",
    "                    'done': torch.tensor(dones, dtype=torch.float32)\n",
    "                }\n",
    "\n",
    "                model.learn(batch)  # Appel à la méthode learn\n",
    "            \n",
    "            # Évaluation (à personnaliser selon vos besoins)\n",
    "            mean_reward = np.random.uniform(-100, 100)  # Remplacez ceci par une évaluation réelle\n",
    "            std_reward = np.random.uniform(0, 50)  # Remplacez ceci par une évaluation réelle\n",
    "            \n",
    "            results.append({**param_dict, 'mean_reward': mean_reward, 'std_reward': std_reward})\n",
    "            print(f\"Récompense moyenne: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec les paramètres {param_dict}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    print(\"Début des tests d'hyperparamètres...\")\n",
    "    results_df = test_hyperparameters()\n",
    "    \n",
    "    # Trier et afficher les résultats\n",
    "    results_df = results_df.sort_values('mean_reward', ascending=False)\n",
    "    print(\"\\nRésultats triés par récompense moyenne:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    results_df.to_csv('td3_hyperparameter_results.csv', index=False)\n",
    "    print(\"\\nRésultats sauvegardés dans 'td3_hyperparameter_results.csv'\")\n",
    "    \n",
    "    # Afficher les meilleurs paramètres\n",
    "    if not results_df.empty:\n",
    "        best_params = results_df.iloc[0].to_dict()\n",
    "        print(\"\\nMeilleurs paramètres trouvés:\")\n",
    "        for key, value in best_params.items():\n",
    "            if key not in ['mean_reward', 'std_reward']:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(f\"Récompense moyenne: {best_params['mean_reward']:.2f} +/- {best_params['std_reward']:.2f}\")\n",
    "    else:\n",
    "        print(\"Aucun résultat valide trouvé.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8fb49-7bdd-4665-af3c-8b8244d6b210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
